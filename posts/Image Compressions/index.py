{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Image Compression\n",
    "author: Phil Chodrow\n",
    "image: \"image.jpg\"\n",
    "description: \"Compression to exact file sizes using machine learning.\"\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I always get annoyed when a website rejects my image because it's 3.2MB instead of the required \"under 2MB\". Most online compression tools let users set quality levels (like JPEG's 0-100 scale), but there's no direct way to say \"make this exactly 350KB.\" This creates a trial-and-error process where users adjust quality settings repeatedly until they hit the target size.\n",
    "In this blog post, I'll explore why exact-size compression is challenging and investigate two approaches: a traditional binary search method and an experimental machine learning technique that attempts to predict optimal compression parameters. The goal is to understand the trade-offs between these methods and whether machine learning can provide any advantages in this space.\n",
    "\n",
    "\n",
    "## Why \"exact size\" compression is difficult\n",
    "To understand the challenge, we need to look at how compression actually works. When we set a JPEG quality to 80, we're not directly controlling the file size - we're adjusting quantization tables that determine how much information gets discarded. The relationship between quality settings and resulting file size is complex and depends on several factors:\n",
    "\n",
    "Content-dependent compression efficiency: A simple logo compressed at quality 80 might be 20KB, while a detailed photograph at the same setting could be 200KB.\n",
    "\n",
    "Quantization steps are discrete: Most compression algorithms use block-based processing (8x8 in JPEG), with each block encoded into variable-length bit streams. This means file size changes in irregular jumps, not continuous increments.\n",
    "\n",
    "Metadata overhead: EXIF data and other metadata can add 10-40KB to file size with no visual impact.\n",
    "Complex optimization space: The mapping between quality settings and file size is non-linear and varies by image content.\n",
    "\n",
    "Let's formulate this mathematically. For an image II\n",
    "I, compressed with parameter qq\n",
    "q (quality), the resulting size SS\n",
    "S is:\n",
    "\n",
    "# $S(I,q) = f(I,q) + M(I)$\n",
    "\n",
    "Where \n",
    "f is a non-linear function dependent on image content, and M(I) is metadata overhead. Our goal is to solve for q when S is our target size:\n",
    "\n",
    "#  Q = f^{-1}(S - M(I), I)$\n",
    "\n",
    "The problem is that $F^-1$ doesn't have a closed-form solution, and worse, it's different for every image.\n",
    "\n",
    "## The common approach: Binary search\n",
    "The standard industry solution is a simple binary search through quality values. Let's implement this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "def encode_jpeg(image, quality):\n",
    "    \"\"\"Encode PIL Image as JPEG with specified quality\"\"\"\n",
    "    # Convert RGBA to RGB if needed\n",
    "    if image.mode == 'RGBA':\n",
    "        background = Image.new('RGB', image.size, (255, 255, 255))\n",
    "        background.paste(image, mask=image.split()[3])\n",
    "        image = background\n",
    "        \n",
    "    buffer = io.BytesIO()\n",
    "    image.save(buffer, format=\"JPEG\", quality=quality)\n",
    "    return buffer.getvalue()\n",
    "\n",
    "def compress_to_target_size(image, target_bytes, q_min=5, q_max=95, tolerance=1024):\n",
    "    \"\"\"Compress image to target size within tolerance\"\"\"\n",
    "    best_result = None\n",
    "    best_size = float('inf')\n",
    "    iterations = 0\n",
    "    \n",
    "    while q_min <= q_max:\n",
    "        iterations += 1\n",
    "        q = (q_min + q_max) // 2  # Binary search approach\n",
    "        compressed = encode_jpeg(image, quality=q)\n",
    "        size = len(compressed)\n",
    "        \n",
    "        print(f\"Iteration: Quality {q} -> Size {size/1024:.1f}KB\")\n",
    "        \n",
    "        # Always keep track of closest result that's under target\n",
    "        if size <= target_bytes and (target_bytes - size < target_bytes - best_size or best_size > target_bytes):\n",
    "            best_result = compressed\n",
    "            best_size = size\n",
    "        \n",
    "        if size > target_bytes:  # Too big, reduce quality\n",
    "            q_max = q - 1\n",
    "        else:  # Fits or too small, try higher quality\n",
    "            q_min = q + 1\n",
    "            \n",
    "    if best_result is None:\n",
    "        print(f\"Warning: Could not compress below target ({target_bytes/1024:.1f}KB). Best result: {best_size/1024:.1f}KB\")\n",
    "        # Return the smallest possible result anyway\n",
    "        return encode_jpeg(image, quality=q_min)\n",
    "        \n",
    "    print(f\"Success! Compressed to {best_size/1024:.1f}KB in {iterations} iterations\")\n",
    "    return best_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this approach works, it has limitations:\n",
    "\n",
    "1) Requires multiple compression attempts (log₂(quality_range) ≈ 6-7)\n",
    "2) May fail to converge if tolerance is too tight\n",
    "3) No learning - repeats the same process for every image\n",
    "\n",
    "Let's test this on a sample image:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: Quality 50 -> Size 72.0KB\n",
      "Iteration: Quality 73 -> Size 104.5KB\n",
      "Iteration: Quality 84 -> Size 148.1KB\n",
      "Iteration: Quality 90 -> Size 200.3KB\n",
      "Iteration: Quality 87 -> Size 167.9KB\n",
      "Iteration: Quality 85 -> Size 152.5KB\n",
      "Success! Compressed to 148.1KB in 6 iterations\n",
      "Success! Final size: 148.1KB\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "image = Image.open(\"sample.jpg\")\n",
    "target_size = 150 * 1024  # 150KB\n",
    "result = compress_to_target_size(image, target_size)\n",
    "\n",
    "if result:\n",
    "    print(f\"Success! Final size: {len(result)/1024:.1f}KB\")\n",
    "    with open(\"output.jpg\", \"wb\") as f:\n",
    "        f.write(result)\n",
    "else:\n",
    "    print(\"Failed to meet target size within tolerance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we successfully found a file under our 150KB limit (at 148.1KB), the binary search explored solutions that were significantly smaller (as low as 72.0KB at quality 50). These smaller files would technically satisfy the \"under limit\" requirement, but at a substantial cost to image quality. A 72KB JPEG will show noticeable compression artifacts compared to the 148KB version, which preserves much more detail.\n",
    "\n",
    "Our current implementation prioritizes getting as close to the limit as possible while staying under it, which is typically the ideal strategy - use the maximum allowed quality within the size constraint. However, this approach has limitations:\n",
    "\n",
    "The binary search can be inefficient, often making dramatic quality jumps between iterations\n",
    "There's no guarantee of visual quality preservation beyond what the quality parameter roughly indicates\n",
    "The relationship between quality settings and file size is highly non-linear and image-dependent\n",
    "\n",
    "A more sophisticated approach would incorporate a quality floor parameter to avoid unnecessarily degrading images when the target size is generous relative to the image content. This would prevent the algorithm from exploring very low-quality settings when higher quality options are available within the size limit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A smarter approach: Machine learning prediction\n",
    "I was curious whether machine learning could improve this process by predicting the optimal quality setting directly. Let me explain the concept:\n",
    "\n",
    "### What is a prediction model?\n",
    "\n",
    "A prediction model in this context is an algorithm that learns the relationship between:\n",
    "1. **Input features**: Characteristics of the image (like edge density, entropy, etc.) and a desired target size\n",
    "2. **Output**: The optimal JPEG quality setting that would produce that target size\n",
    "\n",
    "Essentially, we're trying to approximate the inverse of the compression function. Instead of repeatedly compressing an image at different qualities to find the right size, we want the model to \"guess\" the correct quality parameter in one shot.\n",
    "\n",
    "The process involves:\n",
    "\n",
    "1. **Training**: We collect examples of images compressed at various quality settings, extracting features and recording the resulting file sizes\n",
    "2. **Learning**: The model identifies patterns between image features, quality settings, and file sizes\n",
    "3. **Prediction**: When given a new image and target size, the model estimates the quality setting that would produce that size\n",
    "\n",
    "For this experiment, I've chosen to use a Gradient Boosting Regressor, which is well-suited for this type of non-linear regression problem. It works by building multiple decision trees sequentially, with each tree correcting errors made by the previous ones.\n",
    "\n",
    "### Why might this be better than binary search?\n",
    "\n",
    "If successful, this approach could:\n",
    "- Reduce the number of compression attempts needed (potentially just one if the prediction is accurate)\n",
    "- Provide more consistent results across different types of images\n",
    "- Better handle the non-linear relationship between quality and file size\n",
    "\n",
    "Let's extract features from images that correlate with compression efficiency:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "\n",
    "def extract_features(image):\n",
    "    \"\"\"Extract compression-relevant features from an image\"\"\"\n",
    "    # Convert to numpy array if needed\n",
    "    if isinstance(image, Image.Image):\n",
    "        img_array = np.array(image)\n",
    "    else:\n",
    "        img_array = image\n",
    "    \n",
    "    # Basic dimensions\n",
    "    height, width = img_array.shape[:2]\n",
    "    channels = 1 if len(img_array.shape) == 2 else img_array.shape[2]\n",
    "    \n",
    "    # Calculate edge density using Sobel filter\n",
    "    if channels > 1:\n",
    "        gray = np.mean(img_array, axis=2).astype(np.uint8)\n",
    "    else:\n",
    "        gray = img_array\n",
    "    \n",
    "    sx = ndimage.sobel(gray, axis=0)\n",
    "    sy = ndimage.sobel(gray, axis=1)\n",
    "    edges = np.hypot(sx, sy)\n",
    "    edge_density = np.mean(edges) / 255.0\n",
    "    \n",
    "    # Calculate entropy (measure of information content)\n",
    "    hist, _ = np.histogram(gray.flatten(), bins=256, range=[0, 256])\n",
    "    hist_norm = hist / (hist.sum() + 1e-10)  # Avoid division by zero\n",
    "    non_zero_hist = hist_norm[hist_norm > 0]\n",
    "    entropy = -np.sum(non_zero_hist * np.log2(non_zero_hist))\n",
    "    \n",
    "    # Calculate variance (measure of detail)\n",
    "    variance = np.var(gray) / 255.0\n",
    "    \n",
    "    # Measure of unique pixels (compression difficulty)\n",
    "    unique_ratio = len(np.unique(gray)) / 256.0\n",
    "    \n",
    "    return {\n",
    "        'width': width,\n",
    "        'height': height,\n",
    "        'aspect_ratio': width / height,\n",
    "        'channels': channels,\n",
    "        'edge_density': edge_density,\n",
    "        'entropy': entropy,\n",
    "        'variance': variance,\n",
    "        'unique_ratio': unique_ratio,\n",
    "        'size_raw': width * height * channels\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These features capture different aspects of image complexity that affect compression:\n",
    "\n",
    "Edge density (more edges → harder to compress)\n",
    "Entropy (higher information content → larger compressed size)\n",
    "Variance (more variance → more detail → larger files)\n",
    "Unique pixel ratio (more unique values → less compression potential)\n",
    "\n",
    "Now, let's build a prediction model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def train_size_predictor(image_files, quality_range=range(5, 96, 10)):\n",
    "    \"\"\"Train a model to predict compressed size from image features and quality\"\"\"\n",
    "    data = []\n",
    "    \n",
    "    print(f\"Training model on {len(image_files)} images...\")\n",
    "    for i, img_path in enumerate(image_files):\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Processing image {i}/{len(image_files)}\")\n",
    "            \n",
    "        try:\n",
    "            img = Image.open(img_path)\n",
    "            features = extract_features(img)\n",
    "            \n",
    "            # Compress at different quality levels\n",
    "            for quality in quality_range:\n",
    "                compressed = encode_jpeg(img, quality)\n",
    "                size = len(compressed)\n",
    "                \n",
    "                # Store features, quality, and resulting size\n",
    "                sample = features.copy()\n",
    "                sample['quality'] = quality\n",
    "                sample['compressed_size'] = size\n",
    "                sample['compression_ratio'] = size / features['size_raw']\n",
    "                data.append(sample)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {e}\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Train model to predict compression ratio from features + quality\n",
    "    X = df.drop(['compressed_size', 'compression_ratio'], axis=1)\n",
    "    y = df['compression_ratio']  # Predict ratio rather than absolute size\n",
    "    \n",
    "    model = GradientBoostingRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.1\n",
    "    )\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Print feature importance\n",
    "    importance = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(\"Top 5 features for prediction:\")\n",
    "    print(importance.head(5))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the critical function - predicting quality for a target size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_quality_for_size(model, image, target_bytes):\n",
    "    \"\"\"Predict quality setting needed to achieve target file size\"\"\"\n",
    "    features = extract_features(image)\n",
    "    raw_size = features['size_raw']\n",
    "    target_ratio = target_bytes / raw_size\n",
    "    \n",
    "    # Since we're predicting compression ratio, we need to find\n",
    "    # which quality gives the closest ratio to our target\n",
    "    best_quality = None\n",
    "    min_diff = float('inf')\n",
    "    \n",
    "    for quality in range(5, 96):\n",
    "        test_features = features.copy()\n",
    "        test_features['quality'] = quality\n",
    "        # Convert to DataFrame format\n",
    "        test_df = pd.DataFrame([test_features])\n",
    "        \n",
    "        # Predict compression ratio for this quality\n",
    "        predicted_ratio = model.predict(test_df)[0]\n",
    "        predicted_size = predicted_ratio * raw_size\n",
    "        \n",
    "        diff = abs(predicted_size - target_bytes)\n",
    "        if diff < min_diff:\n",
    "            min_diff = diff\n",
    "            best_quality = quality\n",
    "    \n",
    "    return best_quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's combine our ML prediction with binary search for fine-tuning:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_compress_to_size(model, image, target_bytes, tolerance=1024):\n",
    "    \"\"\"Compress image to target size using ML prediction + binary search refinement\"\"\"\n",
    "    # Step 1: Predict quality using ML model\n",
    "    predicted_quality = predict_quality_for_size(model, image, target_bytes)\n",
    "    \n",
    "    # Step 2: Try the predicted quality\n",
    "    compressed = encode_jpeg(image, quality=predicted_quality)\n",
    "    size = len(compressed)\n",
    "    \n",
    "    print(f\"ML predicted quality: {predicted_quality} -> Size: {size/1024:.1f}KB\")\n",
    "    \n",
    "    # Step 3: If we're close enough, we're done\n",
    "    if abs(size - target_bytes) <= tolerance:\n",
    "        return compressed\n",
    "    \n",
    "    # Step 4: Not close enough, refine with narrow binary search\n",
    "    print(\"Fine-tuning with binary search...\")\n",
    "    search_range = 10\n",
    "    if size > target_bytes:\n",
    "        # Too big, search lower qualities\n",
    "        q_min, q_max = max(5, predicted_quality - search_range), predicted_quality - 1\n",
    "    else:\n",
    "        # Too small, search higher qualities\n",
    "        q_min, q_max = predicted_quality + 1, min(95, predicted_quality + search_range)\n",
    "    \n",
    "    # Use regular binary search in narrower range\n",
    "    return compress_to_target_size(\n",
    "        image, \n",
    "        target_bytes,\n",
    "        q_min=q_min,\n",
    "        q_max=q_max\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments and results\n",
    "Now let's evaluate our approach with different image types. I collected a dataset of 1,000 diverse images and trained our model using a 80/20 train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_compression_methods(test_images, model, target_kb=200):\n",
    "    \"\"\"Compare binary search and ML-assisted compression with simple metrics\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, img_path in enumerate(test_images[:3]):  # Just test 3 images\n",
    "        img = Image.open(img_path)\n",
    "        target_bytes = target_kb * 1024\n",
    "        \n",
    "        # Binary search method\n",
    "        bs_start = time.time()\n",
    "        bs_iterations = [0]  # Use a list to track iterations inside function\n",
    "        \n",
    "        def count_iteration(*args):\n",
    "            bs_iterations[0] += 1\n",
    "            return True\n",
    "            \n",
    "        with patch('builtins.print', side_effect=count_iteration):  # Suppress output\n",
    "            bs_result = compress_to_target_size(img, target_bytes)\n",
    "        \n",
    "        bs_time = time.time() - bs_start\n",
    "        bs_error = abs(len(bs_result) - target_bytes) / target_bytes * 100\n",
    "        \n",
    "        # ML method\n",
    "        ml_start = time.time()\n",
    "        ml_iterations = [0]\n",
    "        \n",
    "        with patch('builtins.print', side_effect=count_iteration):\n",
    "            ml_result = smart_compress_to_size(model, img, target_bytes)\n",
    "            \n",
    "        ml_time = time.time() - ml_start\n",
    "        ml_error = abs(len(ml_result) - target_bytes) / target_bytes * 100\n",
    "        \n",
    "        # Save results\n",
    "        results.append({\n",
    "            'image': os.path.basename(img_path),\n",
    "            'bs_iterations': bs_iterations[0],\n",
    "            'ml_iterations': ml_iterations[0],\n",
    "            'bs_time': bs_time,\n",
    "            'ml_time': ml_time,\n",
    "            'bs_error': bs_error,\n",
    "            'ml_error': ml_error,\n",
    "            'speedup': bs_time / ml_time\n",
    "        })\n",
    "    \n",
    "    # Print results in a clean table\n",
    "    print(f\"{'Image':<20} {'BS Iter':^8} {'ML Iter':^8} {'BS Time':^8} {'ML Time':^8} {'BS Err%':^8} {'ML Err%':^8} {'Speedup':^8}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    avg_speedup = 0\n",
    "    for result in results:\n",
    "        print(f\"{result['image']:<20} {result['bs_iterations']:^8} {result['ml_iterations']:^8} \"\n",
    "              f\"{result['bs_time']:.2f}s {result['ml_time']:.2f}s {result['bs_error']:.2f}% \"\n",
    "              f\"{result['ml_error']:.2f}% {result['speedup']:.2f}x\")\n",
    "        avg_speedup += result['speedup']\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Average speedup: {avg_speedup/len(results):.2f}x\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image                BS Iter  ML Iter  BS Time  ML Time  BS Err%  ML Err%  Speedup \n",
      "--------------------------------------------------------------------------------\n",
      "IMG_1281.jpeg           14       0     1.13s 1.84s 0.59% 71.11% 0.62x\n",
      "IMG_1217.jpeg           10       0     2.11s 2.95s 106.04% 106.04% 0.72x\n",
      "IMG_1946.jpeg           10       0     1.29s 1.50s 95.53% 95.53% 0.86x\n",
      "--------------------------------------------------------------------------------\n",
      "Average speedup: 0.73x\n"
     ]
    }
   ],
   "source": [
    "from unittest.mock import patch\n",
    "results = evaluate_compression_methods(test_files, model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize these results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot time comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(df['binary_time'], df['smart_time'])\n",
    "plt.plot([0, df['binary_time'].max()], [0, df['binary_time'].max()], 'r--')\n",
    "plt.xlabel('Binary Search Time (s)')\n",
    "plt.ylabel('ML-Assisted Time (s)')\n",
    "plt.title('Compression Time Comparison')\n",
    "\n",
    "# Plot error comparison\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(df['binary_accuracy'], df['smart_accuracy'])\n",
    "plt.plot([0, df['binary_accuracy'].max()], [0, df['binary_accuracy'].max()], 'r--')\n",
    "plt.xlabel('Binary Search Error (%)')\n",
    "plt.ylabel('ML-Assisted Error (%)')\n",
    "plt.title('Compression Accuracy Comparison')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-0451",
   "language": "python",
   "name": "ml-0451"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "debe06cc0f9553f110b64dc3926c05df82dae2145b852c8422b9c04315589dcb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
